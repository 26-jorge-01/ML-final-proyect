{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entidades = pd.read_csv(\"../data/bronze/entidades_proyecto.csv\", sep=';')\n",
    "# Se suprime la variable ID para la cosntrucción de modelos debido a que no se sabe si la etiqueta está bien asignada o no\n",
    "entidades = entidades.drop(columns=['ID'])\n",
    "\n",
    "# Dado que los códigos se generan para cada registro, no se tomarán para el modelo debido a que estos se generan unicamente después del registro en alguna de las plataformas y el objetivo del proyecto\n",
    "# es desarrollar el mecanismo para que se valide si ya existe antes de que sea registrado\n",
    "entidades = entidades.drop(columns=['CODIGO'])\n",
    "\n",
    "entidades['NOMBRE'] = entidades['NOMBRE'].astype(str)\n",
    "entidades['NIT'] = entidades['NIT'].astype(str)\n",
    "\n",
    "entidades.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crearán 10 conjuntos de entrenamiento\n",
    "train_sets = {}\n",
    "\n",
    "for i in range(10):\n",
    "    train_sets[i] = entidades.sample(frac=0.7, replace=True, random_state=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Se implementará un modelo preentrenado multilenguaje para la creación de los vectores de los registros tanto de nombres como de nits\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_names_sets = {}\n",
    "input_nits_sets = {}\n",
    "\n",
    "for train_set in train_sets:\n",
    "    input_names_sets[train_set] = tokenizer(train_sets[train_set]['NOMBRE'].to_list(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    input_nits_sets[train_set] = tokenizer(train_sets[train_set]['NIT'].to_list(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "names_embebings_sets = {}\n",
    "nits_embedings_sets = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Nombres\n",
    "    for input_set in input_names_sets:\n",
    "        names_embebings_sets[input_set] = model(**input_names_sets[input_set]).logits.numpy()\n",
    "    \n",
    "    # Nits\n",
    "    for input_set in input_nits_sets:\n",
    "        nits_embedings_sets[input_set] = model(**input_nits_sets[input_set]).logits.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarización de los vectores\n",
    "names_embebings_sets_standardized = {}\n",
    "nits_embebings_sets_standardized = {}\n",
    "\n",
    "scaler_names = StandardScaler()\n",
    "for names_embeging_set in names_embebings_sets:\n",
    "    names_embebings_sets_standardized[names_embeging_set] = scaler_names.fit_transform(names_embebings_sets[names_embeging_set])\n",
    "\n",
    "scaler_nits = StandardScaler()\n",
    "for nits_embeging_set in nits_embedings_sets:\n",
    "    nits_embebings_sets_standardized[nits_embeging_set] = scaler_nits.fit_transform(nits_embedings_sets[nits_embeging_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('names_embebings_sets_standardized:', names_embebings_sets_standardized[0].shape)\n",
    "print('nits_embebings_sets_standardized:', nits_embebings_sets_standardized[0].shape)\n",
    "\n",
    "entidades_embebidas = {}\n",
    "for train_set in train_sets:\n",
    "    names_embebings_sets_standardized[train_set] = names_embebings_sets_standardized[train_set].astype(float)\n",
    "    nits_embebings_sets_standardized[train_set] = nits_embebings_sets_standardized[train_set].astype(float)\n",
    "    entidades_embebidas[train_set] = np.concatenate((names_embebings_sets_standardized[train_set], nits_embebings_sets_standardized[train_set]), axis=1)\n",
    "\n",
    "print('entidades_embebidas:', entidades_embebidas[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponemos que estos son tus cuatro DataFrames\n",
    "dataframes_list = [[names_embebings_sets_standardized[x], nits_embebings_sets_standardized[x], entidades_embebidas[x]] for x in train_sets.keys()]\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for dataframe_list in dataframes_list:\n",
    "    for i, df in enumerate(dataframe_list, 1):\n",
    "        pca = PCA().fit(df)\n",
    "        plt.subplot(1, 3, i)\n",
    "        plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "        plt.xlabel('Número de Componentes')\n",
    "        plt.ylabel('Varianza Explicada')\n",
    "        plt.title(f'Análisis de Varianza Explicada para DataFrame {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que para todos los data sets de entrenamiento se logra capturar un 95% de la varianza en el caso de los nombres y los nits por separado, mientras que en el caso de los datasets que resultan de haber unido las vectorizaciones de nombres y nits, es necesario contar con 3 componentes principales para alcanzar el punto de codo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_embebings_sets_pca = {}\n",
    "nits_embebings_sets_pca = {}\n",
    "entidades_embebidas_pca = {}\n",
    "\n",
    "names_pca = PCA(n_components=2)\n",
    "nits_pca = PCA(n_components=2)\n",
    "entidades_pca = PCA(n_components=3)\n",
    "\n",
    "for train_set in train_sets:\n",
    "    names_embebings_sets_pca[train_set] = pd.DataFrame(names_pca.fit_transform(names_embebings_sets_standardized[train_set]))\n",
    "    for col in names_embebings_sets_pca[train_set].columns:\n",
    "        names_embebings_sets_pca[train_set][col] = names_embebings_sets_pca[train_set][col].astype(float)\n",
    "\n",
    "    nits_embebings_sets_pca[train_set] = pd.DataFrame(nits_pca.fit_transform(nits_embebings_sets_standardized[train_set]))\n",
    "    for col in nits_embebings_sets_pca[train_set].columns:\n",
    "        nits_embebings_sets_pca[train_set][col] = nits_embebings_sets_pca[train_set][col].astype(float)\n",
    "\n",
    "    entidades_embebidas_pca[train_set] = pd.DataFrame(entidades_pca.fit_transform(entidades_embebidas[train_set]))\n",
    "    for col in entidades_embebidas_pca[train_set].columns:\n",
    "        entidades_embebidas_pca[train_set][col] = entidades_embebidas_pca[train_set][col].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = range(2, 25)  # El score de silueta no se puede calcular para Nc = 1\n",
    "\n",
    "# Configuramos el tamaño de la figura para visualizar todas las gráficas adecuadamente\n",
    "plt.figure(figsize=(30, 25))\n",
    "\n",
    "# Iterar sobre las llaves de los diccionarios\n",
    "for i, key in enumerate(names_embebings_sets_pca.keys(), 1):\n",
    "    dataframes = [names_embebings_sets_pca[key], nits_embebings_sets_pca[key], entidades_embebidas_pca[key]]\n",
    "    \n",
    "    # Iterar sobre cada DataFrame y aplicar PCA\n",
    "    for j, df in enumerate(dataframes):\n",
    "        scores = []\n",
    "        sil_scores = []\n",
    "        kmeans = [KMeans(n_clusters=num_clusters) for num_clusters in Nc]\n",
    "        \n",
    "        for model in kmeans:\n",
    "            model.fit(df)\n",
    "            scores.append(-model.score(df))  # Usamos el negativo del score porque KMeans minimiza la inercia\n",
    "            labels = model.labels_\n",
    "            sil_score = silhouette_score(df, labels)\n",
    "            sil_scores.append(sil_score)\n",
    "        \n",
    "        # Crear un subplot para cada DataFrame\n",
    "        ax = plt.subplot(len(names_embebings_sets_pca), 3, (i-1)*3 + j + 1)\n",
    "        ax.plot(Nc, scores, marker='o', linestyle='--', color='blue', label='Elbow Score')\n",
    "        ax.set_xlabel('Número de Clusters')\n",
    "        ax.set_ylabel('Inertia', color='blue')\n",
    "        ax.tick_params(axis='y', labelcolor='blue')\n",
    "        ax.set_title(f'Elbow Curve y Silhouette Score para {key} DataFrame {j+1}')\n",
    "\n",
    "        # Crear eje y secundario para los scores de silueta\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(Nc, sil_scores, marker='o', linestyle='--', color='red', label='Silhouette Score')\n",
    "        ax2.set_ylabel('Silhouette Score', color='red')\n",
    "        ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "        # Añadir leyendas\n",
    "        ax.legend(loc='upper left')\n",
    "        ax2.legend(loc='upper right')\n",
    "\n",
    "# Mostrar todas las gráficas\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos el tamaño de la figura para visualizar todas las gráficas adecuadamente\n",
    "plt.figure(figsize=(30, 25))\n",
    "\n",
    "# Iterar sobre las llaves de los diccionarios\n",
    "for i, key in enumerate(names_embebings_sets_pca.keys(), 1):\n",
    "    dataframes = [names_embebings_sets_pca[key], nits_embebings_sets_pca[key], entidades_embebidas_pca[key]]\n",
    "    \n",
    "    # Iterar sobre cada DataFrame y aplicar PCA\n",
    "    for j, df in enumerate(dataframes):\n",
    "        scores = []\n",
    "        sil_scores = []\n",
    "        gmm_list = [GaussianMixture(n_components=num_clusters, random_state=0) for num_clusters in Nc]\n",
    "        \n",
    "        for gmm in gmm_list:\n",
    "            gmm.fit(df)\n",
    "            labels = gmm.predict(df)\n",
    "            scores.append(-gmm.score(df) * len(df))  # Usamos el negativo del score porque GaussianMixture maximiza el log likelihood\n",
    "            sil_score = silhouette_score(df, labels)\n",
    "            sil_scores.append(sil_score)\n",
    "        \n",
    "        # Crear un subplot para cada DataFrame\n",
    "        ax = plt.subplot(len(names_embebings_sets_pca), 3, (i-1)*3 + j + 1)\n",
    "        ax.plot(Nc, scores, marker='o', linestyle='--', color='blue', label='Elbow Score')\n",
    "        ax.set_xlabel('Número de Clusters')\n",
    "        ax.set_ylabel('Inertia', color='blue')\n",
    "        ax.tick_params(axis='y', labelcolor='blue')\n",
    "        ax.set_title(f'Elbow Curve y Silhouette Score para {key} DataFrame {j+1}')\n",
    "\n",
    "        # Crear eje y secundario para los scores de silueta\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(Nc, sil_scores, marker='o', linestyle='--', color='red', label='Silhouette Score')\n",
    "        ax2.set_ylabel('Silhouette Score', color='red')\n",
    "        ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "        # Añadir leyendas\n",
    "        ax.legend(loc='upper left')\n",
    "        ax2.legend(loc='upper right')\n",
    "\n",
    "# Mostrar todas las gráficas\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_names = ['Nombres', 'NITs', 'Entidades']\n",
    "\n",
    "# Definir rangos para los parámetros de HDBSCAN\n",
    "min_cluster_sizes = range(4, 11)\n",
    "min_samples_list = range(1, 6)\n",
    "\n",
    "# Inicializar DataFrames para almacenar la suma de los resultados por columna\n",
    "average_results_per_column = [pd.DataFrame(index=min_cluster_sizes, columns=min_samples_list, dtype=float).fillna(0) for _ in range(len(dataframe_names))]\n",
    "count_per_column = [0] * len(dataframe_names)\n",
    "\n",
    "# Preparar figura para los heatmaps\n",
    "fig, axes = plt.subplots(len(names_embebings_sets_pca) + 1, len(dataframe_names), figsize=(25, 35))  # +1 fila para el heatmap promedio\n",
    "\n",
    "# Iterar sobre las llaves de los diccionarios\n",
    "for i, key in enumerate(names_embebings_sets_pca.keys()):\n",
    "    dataframes = [names_embebings_sets_pca[key], nits_embebings_sets_pca[key], entidades_embebidas_pca[key]]\n",
    "    \n",
    "    # Iterar sobre cada DataFrame\n",
    "    for j, (df, name) in enumerate(zip(dataframes, dataframe_names)):\n",
    "        results = pd.DataFrame(index=min_cluster_sizes, columns=min_samples_list, dtype=float)\n",
    "\n",
    "        for min_cluster_size in min_cluster_sizes:\n",
    "            for min_samples in min_samples_list:\n",
    "                # Configurar HDBSCAN\n",
    "                clusterer = HDBSCAN(min_cluster_size=min_cluster_size, \n",
    "                                    min_samples=min_samples, \n",
    "                                    metric='manhattan', \n",
    "                                    cluster_selection_method='eom')\n",
    "                labels = clusterer.fit_predict(df)\n",
    "                \n",
    "                # Calcular Silhouette Score solo si se forman al menos 2 clusters (excluyendo el ruido)\n",
    "                if len(set(labels)) > 1:\n",
    "                    sil_score = silhouette_score(df, labels)\n",
    "                else:\n",
    "                    sil_score = -1  # Indicativo de un resultado no útil o no clusterizado\n",
    "                \n",
    "                results.at[min_cluster_size, min_samples] = sil_score\n",
    "        \n",
    "        # Asegurarse de que todos los valores sean float\n",
    "        results = results.astype(float)\n",
    "\n",
    "        # Acumular los resultados para el promedio por columna\n",
    "        average_results_per_column[j] += results\n",
    "        count_per_column[j] += 1\n",
    "\n",
    "        # Mostrar heatmap para cada DataFrame\n",
    "        sns.heatmap(results, annot=True, ax=axes[i, j], cmap='viridis', fmt=\".2f\")\n",
    "        axes[i, j].set_title(f'Silhouette Scores Heatmap for {key} - {name}')\n",
    "        axes[i, j].set_xlabel('Min Samples')\n",
    "        axes[i, j].set_ylabel('Min Cluster Size')\n",
    "\n",
    "# Calcular el promedio de los resultados por columna\n",
    "for j, name in enumerate(dataframe_names):\n",
    "    average_results_per_column[j] /= count_per_column[j]\n",
    "    sns.heatmap(average_results_per_column[j], annot=True, ax=axes[-1, j], cmap='viridis', fmt=\".2f\")\n",
    "    axes[-1, j].set_title(f'Average Silhouette Scores Heatmap for {name}')\n",
    "    axes[-1, j].set_xlabel('Min Samples')\n",
    "    axes[-1, j].set_ylabel('Min Cluster Size')\n",
    "\n",
    "# Ajustar el layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para establecer la cantidad de clusters usaremos las mejores dos puntuaciones de silueta evaluadas en los tres tipos de conjuntos de datos sobre los cuales estamos especializando a nuestros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for i, key in enumerate(names_embebings_sets_pca.keys()):\n",
    "    dataframes = [names_embebings_sets_pca[key]] # , nits_embebings_sets_pca[key], entidades_embebidas_pca[key]\n",
    "    \n",
    "    # Iterar sobre cada DataFrame\n",
    "    for j, (df, name) in enumerate(zip(dataframes, dataframe_names)):\n",
    "        results = pd.DataFrame(index=min_cluster_sizes, columns=min_samples_list, dtype=float)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=4, min_samples=1, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=4, min_samples=2, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=4, min_samples=3, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=6, min_samples=1, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=6, min_samples=2, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "lens = pd.DataFrame(lens)\n",
    "print('Cantidad de clusters para nombres:', lens.mean().values[0])\n",
    "\n",
    "lens = []\n",
    "for i, key in enumerate(names_embebings_sets_pca.keys()):\n",
    "    dataframes = [nits_embebings_sets_pca[key]] # , nits_embebings_sets_pca[key], entidades_embebidas_pca[key]\n",
    "    \n",
    "    # Iterar sobre cada DataFrame\n",
    "    for j, (df, name) in enumerate(zip(dataframes, dataframe_names)):\n",
    "        results = pd.DataFrame(index=min_cluster_sizes, columns=min_samples_list, dtype=float)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=4, min_samples=1, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=4, min_samples=2, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=4, min_samples=3, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=4, min_samples=4, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=5, min_samples=1, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=5, min_samples=2, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=5, min_samples=3, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=5, min_samples=4, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "lens = pd.DataFrame(lens)\n",
    "print('Cantidad de clusters para nits:', lens.mean().values[0])\n",
    "\n",
    "lens = []\n",
    "for i, key in enumerate(names_embebings_sets_pca.keys()):\n",
    "    dataframes = [entidades_embebidas_pca[key]] # , nits_embebings_sets_pca[key], entidades_embebidas_pca[key]\n",
    "    \n",
    "    # Iterar sobre cada DataFrame\n",
    "    for j, (df, name) in enumerate(zip(dataframes, dataframe_names)):\n",
    "        results = pd.DataFrame(index=min_cluster_sizes, columns=min_samples_list, dtype=float)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=4, min_samples=1, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=4, min_samples=2, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=7, min_samples=1, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=7, min_samples=2, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=8, min_samples=1, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "        clusterer = HDBSCAN(min_cluster_size=8, min_samples=2, metric='manhattan', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(df)\n",
    "        lens.append(len(list(set(labels))) - 1)\n",
    "\n",
    "lens = pd.DataFrame(lens)\n",
    "print('Cantidad de clusters para nombres y nits:', lens.mean().values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la información anterior, crearemos modelos que se especialicen en la información que va diligenciando el usuario. Esto significa que si el usuario solo diligencia el nombre, se usará solo el dataset de nombres y de igual forma si diligencia solo el nit o si diligencia el nombre y el nit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_datasets_clusterized = []\n",
    "\n",
    "for i, key in enumerate(names_embebings_sets_pca.keys(), 1):\n",
    "    dataframes = [names_embebings_sets_pca[key]]\n",
    "    for j, df in enumerate(dataframes):\n",
    "        # Asegurarse de que todos los nombres de columnas sean cadenas\n",
    "        df.columns = df.columns.astype(str)\n",
    "        \n",
    "        # Kmeans\n",
    "        model_km = KMeans(n_clusters=23)\n",
    "        model_km.fit(df)\n",
    "        df['cluster'] = model_km.labels_\n",
    "        names_datasets_clusterized.append(df.copy())\n",
    "        df = df.drop(columns=['cluster'])\n",
    "\n",
    "        # GaussianMixture\n",
    "        gmm = GaussianMixture(n_components=23, random_state=0)\n",
    "        gmm.fit(df)\n",
    "        df['cluster'] = gmm.predict(df)\n",
    "        names_datasets_clusterized.append(df.copy())\n",
    "        df = df.drop(columns=['cluster'])\n",
    "\n",
    "        # HDBSCAN\n",
    "        for min_cluster_size in min_cluster_sizes:\n",
    "            for min_samples in min_samples_list:\n",
    "                # Configurar HDBSCAN\n",
    "                clusterer = HDBSCAN(min_cluster_size=min_cluster_size, \n",
    "                                    min_samples=min_samples, \n",
    "                                    metric='manhattan', \n",
    "                                    cluster_selection_method='eom')\n",
    "                df['cluster'] = clusterer.fit_predict(df)\n",
    "                names_datasets_clusterized.append(df.copy())\n",
    "                df = df.drop(columns=['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nits_datasets_clusterized = []\n",
    "\n",
    "for i, key in enumerate(nits_embebings_sets_pca.keys(), 1):\n",
    "    dataframes = [nits_embebings_sets_pca[key]]\n",
    "    for j, df in enumerate(dataframes):\n",
    "        # Asegurarse de que todos los nombres de columnas sean cadenas\n",
    "        df.columns = df.columns.astype(str)\n",
    "        \n",
    "        # Kmeans\n",
    "        model_km = KMeans(n_clusters=18)\n",
    "        model_km.fit(df)\n",
    "        df['cluster'] = model_km.labels_\n",
    "        nits_datasets_clusterized.append(df.copy())\n",
    "        df = df.drop(columns=['cluster'])\n",
    "\n",
    "        # GaussianMixture\n",
    "        gmm = GaussianMixture(n_components=18, random_state=0)\n",
    "        gmm.fit(df)\n",
    "        df['cluster'] = gmm.predict(df)\n",
    "        nits_datasets_clusterized.append(df.copy())\n",
    "        df = df.drop(columns=['cluster'])\n",
    "\n",
    "        # HDBSCAN\n",
    "        for min_cluster_size in min_cluster_sizes:\n",
    "            for min_samples in min_samples_list:\n",
    "                # Configurar HDBSCAN\n",
    "                clusterer = HDBSCAN(min_cluster_size=min_cluster_size, \n",
    "                                    min_samples=min_samples, \n",
    "                                    metric='manhattan', \n",
    "                                    cluster_selection_method='eom')\n",
    "                df['cluster'] = clusterer.fit_predict(df)\n",
    "                nits_datasets_clusterized.append(df.copy())\n",
    "                df = df.drop(columns=['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_datasets_clusterized = []\n",
    "\n",
    "for i, key in enumerate(entidades_embebidas_pca.keys(), 1):\n",
    "    dataframes = [entidades_embebidas_pca[key]]\n",
    "    for j, df in enumerate(dataframes):\n",
    "        # Asegurarse de que todos los nombres de columnas sean cadenas\n",
    "        df.columns = df.columns.astype(str)\n",
    "        \n",
    "        # Kmeans\n",
    "        model_km = KMeans(n_clusters=15)\n",
    "        model_km.fit(df)\n",
    "        df['cluster'] = model_km.labels_\n",
    "        entities_datasets_clusterized.append(df.copy())\n",
    "        df = df.drop(columns=['cluster'])\n",
    "\n",
    "        # GaussianMixture\n",
    "        gmm = GaussianMixture(n_components=15, random_state=0)\n",
    "        gmm.fit(df)\n",
    "        df['cluster'] = gmm.predict(df)\n",
    "        entities_datasets_clusterized.append(df.copy())\n",
    "        df = df.drop(columns=['cluster'])\n",
    "\n",
    "        # HDBSCAN\n",
    "        for min_cluster_size in min_cluster_sizes:\n",
    "            for min_samples in min_samples_list:\n",
    "                # Configurar HDBSCAN\n",
    "                clusterer = HDBSCAN(min_cluster_size=min_cluster_size, \n",
    "                                    min_samples=min_samples, \n",
    "                                    metric='manhattan', \n",
    "                                    cluster_selection_method='eom')\n",
    "                df['cluster'] = clusterer.fit_predict(df)\n",
    "                entities_datasets_clusterized.append(df.copy())\n",
    "                df = df.drop(columns=['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de arboles para clasificación\n",
    "\n",
    "Dado que cada uno de los datasets que hemos creado podría o no, capturar el fenómeno que queremos controlar. Crearemos un arbol de desición para clasificación sobreentrenado sobre cada uno de los tres tipos de conjuntos de datos, para predecir el comportamiento de manera similar a como se genera usando bagging: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arboles expertos en nombres\n",
    "names_decision_trees = []\n",
    "\n",
    "for df in names_datasets_clusterized:\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(df[df.columns[:-1]], df['cluster'])\n",
    "    names_decision_trees.append(tree)\n",
    "\n",
    "# Arboles expertos en nits\n",
    "nits_decision_trees = []\n",
    "\n",
    "for df in nits_datasets_clusterized:\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(df[df.columns[:-1]], df['cluster'])\n",
    "    nits_decision_trees.append(tree)\n",
    "\n",
    "# Arboles expertos en entidades (nombres y nits)\n",
    "entities_decision_trees = []\n",
    "\n",
    "for df in entities_datasets_clusterized:\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(df[df.columns[:-1]], df['cluster'])\n",
    "    entities_decision_trees.append(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "def predict_with_names_trees(new_data, names_decision_trees=names_decision_trees):\n",
    "    predictions = []\n",
    "    for tree in names_decision_trees:\n",
    "        pred = tree.predict(new_data)\n",
    "        if pred[0] != -1:\n",
    "            predictions.append(pred[0])\n",
    "    return mode(predictions).mode\n",
    "\n",
    "def predict_with_nits_trees(new_data, nits_decision_trees=nits_decision_trees):\n",
    "    predictions = []\n",
    "    for tree in nits_decision_trees:\n",
    "        pred = tree.predict(new_data)\n",
    "        if pred[0] != -1:\n",
    "            predictions.append(pred[0])\n",
    "    return mode(predictions).mode\n",
    "\n",
    "def predict_with_entities_trees(new_data, entities_decision_trees=entities_decision_trees):\n",
    "    predictions = []\n",
    "    for tree in entities_decision_trees:\n",
    "        pred = tree.predict(new_data)\n",
    "        if pred[0] != -1:\n",
    "            predictions.append(pred[0])\n",
    "    return mode(predictions).mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres_a_revisar = nombres_entidades_prueba = pd.DataFrame(names_pca.transform(scaler_names.transform(model(**tokenizer(entidades['NOMBRE'].to_list(), padding=True, truncation=True, return_tensors=\"pt\")).logits.detach().numpy()).astype(float)))\n",
    "\n",
    "for i, row in nombres_a_revisar.iterrows():\n",
    "    # Convertir la fila a un DataFrame de una sola fila\n",
    "    single_row_df = row.to_frame().T\n",
    "    nombres_a_revisar.at[i, 'grupo'] = predict_with_names_trees(single_row_df)\n",
    "\n",
    "entidades_copy = entidades.copy()\n",
    "entidades['grupo'] = nombres_a_revisar['grupo']\n",
    "entidades.to_csv('../data/gold/entidades_agrupadas_por_nombres.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nits_a_revisar = pd.DataFrame(nits_pca.transform(scaler_nits.transform(model(**tokenizer(entidades['NIT'].to_list(), padding=True, truncation=True, return_tensors=\"pt\")).logits.detach().numpy()).astype(float)))\n",
    "\n",
    "for i, row in nits_a_revisar.iterrows():\n",
    "    # Convertir la fila a un DataFrame de una sola fila\n",
    "    single_row_df = row.to_frame().T\n",
    "    nits_a_revisar.at[i, 'grupo'] = predict_with_nits_trees(single_row_df)\n",
    "\n",
    "entidades_copy = entidades.copy()\n",
    "entidades['grupo'] = nits_a_revisar['grupo']\n",
    "entidades.to_csv('../data/gold/entidades_agrupadas_por_nits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entidades_a_revisar = pd.DataFrame(entidades_pca.transform(np.concatenate((scaler_names.transform(model(**tokenizer(entidades['NOMBRE'].to_list(), padding=True, truncation=True, return_tensors=\"pt\")).logits.detach().numpy()).astype(float), \n",
    "                            scaler_nits.transform(model(**tokenizer(entidades['NIT'].to_list(), padding=True, truncation=True, return_tensors=\"pt\")).logits.detach().numpy()).astype(float)), axis=1)))\n",
    "\n",
    "for i, row in entidades_a_revisar.iterrows():\n",
    "    # Convertir la fila a un DataFrame de una sola fila\n",
    "    single_row_df = row.to_frame().T\n",
    "    entidades_a_revisar.at[i, 'grupo'] = predict_with_entities_trees(single_row_df)\n",
    "\n",
    "entidades_copy = entidades.copy()\n",
    "entidades['grupo'] = entidades_a_revisar['grupo']\n",
    "entidades.to_csv('../data/gold/entidades_agrupadas_por_nombres_y_nits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.DataFrame(names_pca.transform(scaler_names.transform(model(**tokenizer(['sena'], padding=True, truncation=True, return_tensors=\"pt\")).logits.detach().numpy()).astype(float)))\n",
    "classe = predict_with_names_trees(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almacenamiento de modelos\n",
    "import joblib\n",
    "\n",
    "for i, tree in enumerate(names_decision_trees):\n",
    "    joblib.dump(tree, f'../data/gold/model_weights/decision_trees/names/names_tree_{i}.joblib')\n",
    "\n",
    "for i, tree in enumerate(nits_decision_trees):\n",
    "    joblib.dump(tree, f'../data/gold/model_weights/decision_trees/nits/nits_tree_{i}.joblib')\n",
    "\n",
    "for i, tree in enumerate(entities_decision_trees):\n",
    "    joblib.dump(tree, f'../data/gold/model_weights/decision_trees/entities/entities_tree_{i}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('../data/gold/model_weights/tokenizer')\n",
    "tokenizer.save_pretrained('../data/gold/model_weights/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(scaler_names, '../data/gold/model_weights/scalers/scaler_names.joblib')\n",
    "joblib.dump(scaler_nits, '../data/gold/model_weights/scalers/scaler_nits.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(names_pca, '../data/gold/model_weights/pca/names_pca.joblib')\n",
    "joblib.dump(nits_pca, '../data/gold/model_weights/pca/nits_pca.joblib')\n",
    "joblib.dump(entidades_pca, '../data/gold/model_weights/pca/entidades_pca.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
